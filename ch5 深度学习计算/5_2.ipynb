{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d2b1d4-a559-49ba-b989-4c45769ba50f",
   "metadata": {},
   "source": [
    "# 5.2 参数管理\n",
    "我们下面关注操作参数的具体细节：\n",
    "- 访问参数，用于调试，诊断，可视化；\n",
    "- 参数初始化；\n",
    "- 在不同模型组件之间共享参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b75195f-a98e-4beb-a1fa-5dce6aa67732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0356],\n",
       "        [0.0288]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "net = nn.Sequential(nn.Linear(4, 8), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(8, 1))\n",
    "\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99272c15-dc7f-4d12-81b1-6b4cea66f15e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0571,  0.3135, -0.3087, -0.3134,  0.1762, -0.2426, -0.1297, -0.2376]])), ('bias', tensor([0.0630]))])\n"
     ]
    }
   ],
   "source": [
    "## 5.2.1 参数访问\n",
    "print(net[2].state_dict())\n",
    "# 我们在这里可以通过索引访问模型的任意层，就像是列表一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53566a34-22af-4cc3-941a-5af0eee9f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0630], requires_grad=True)\n",
      "tensor([0.0630])\n"
     ]
    }
   ],
   "source": [
    "# 注意每一个参数都表示[参数类]的一个实例，要想操作需要访问底层的数值\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cae2c4-922d-4425-9591-9628b06751df",
   "metadata": {},
   "source": [
    "> 注：参数本身是复合的对象，包含了值，梯度，额外信息等等，有时候我们需要**显式**参数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e28b7b0-b9d0-4d20-808b-9db301c9e3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da1ef31-f95b-44be-97ac-1f5456b5f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "#  *[...]这是python语法：列表推导式\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7f350ba-8564-4ae2-a2c3-12d20a82ab37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0630])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由此可见，我们访问网络参数可借用名称\n",
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58211090-bac6-48a7-b8ae-443c2e8c726a",
   "metadata": {},
   "source": [
    "### 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dd0a904-7c7d-4172-81f6-f237ef4f450f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1934],\n",
       "        [-0.1934]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8), nn.ReLU(),\n",
    "                         nn.Linear(8,4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 进行嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4,1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b759f5c3-e008-4cdf-86d7-a9ee2d19e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)\n",
    "# 这里可以比较显然的看出，模型内部是有类似于递归嵌套的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c102f3a0-7683-4e15-bee4-697444367abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (block 0): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (block 1): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (block 2): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (block 3): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n",
      "-----------------\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "-----------------\n",
      "Linear(in_features=4, out_features=8, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3776,  0.1784,  0.3345,  0.1383],\n",
       "        [ 0.0346, -0.0722,  0.0191,  0.4177],\n",
       "        [-0.2916, -0.0995, -0.4167, -0.1662],\n",
       "        [ 0.3867, -0.4857, -0.3675,  0.4406],\n",
       "        [ 0.1097,  0.2844,  0.1520, -0.1645],\n",
       "        [ 0.0091,  0.0481, -0.4650,  0.4348],\n",
       "        [-0.3623, -0.0237,  0.3339, -0.3785],\n",
       "        [-0.0992,  0.2650, -0.1357,  0.0554]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rgnet[0])\n",
    "print(\"-----------------\")\n",
    "print(rgnet[0][1])\n",
    "print(\"-----------------\")\n",
    "print(rgnet[0][1][0])\n",
    "rgnet[0][1][0].weight.data\n",
    "# in 4 out 8 -> weight:4*8 bias:8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9164a765-204e-46d8-a2c0-36da8bd24d08",
   "metadata": {},
   "source": [
    "## 5.2.2 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58685c90-d46b-46e1-81f1-f6a3d5db1955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0026,  0.0084, -0.0089,  0.0118]), tensor(0.))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 内置初始化\n",
    "def init_normal(m):\n",
    "    \"\"\"高斯随机变量初始化\"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f82dcc0-647b-467c-9e3b-be95b8e5fcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    \"\"\"初始化为常数\"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35f63d8a-15e0-4925-bf04-fb5d6c5d5882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0746, -0.2409,  0.5661,  0.6185])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 我们还可以对不同的块应用不同的初始化方法\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd8e7609-e729-4062-89d1-60249911ded4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000, -6.5335, -0.0000],\n",
       "        [ 0.0000,  9.8022,  8.0159, -0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义初始化\n",
    "# 当我们希望使用特定的分布进行初始化时\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                            for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "480b1287-618f-4d70-ab64-699f3884e95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  1.0000, -5.5335,  1.0000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b861145-7642-46ea-938a-299f854d1049",
   "metadata": {},
   "source": [
    "## 5.2.3 参数绑定\n",
    "有时，我们希望在多个层间**共享参数**\n",
    "比如说：我们可以定义一个稠密层，然后用他的参数来设置另一个层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e893a1e-bec0-44ce-9c2d-5417a750cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要【给共享层一个名称】，以便可以引用他的参数？？\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是[同一个对象]，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67caf5-c8db-4970-8ff7-de85ee050c83",
   "metadata": {},
   "source": [
    "### 练习\n",
    "1. 为什么共享参数是个好主意：\n",
    "   例如卷积神经网络通过使用卷积核共享参数；循环神经网络，transformer模型中的自注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec372d6d-09cd-4956-90b1-d0448f5e3f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
